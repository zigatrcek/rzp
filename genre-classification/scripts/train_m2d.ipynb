{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2D Electronic Music Genre Classification\n",
    "\n",
    "This notebook implements genre classification using M2D embeddings with a linear classifier on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (1.0.12)\n",
      "Requirement already satisfied: einops in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: nnAudio in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: librosa in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: wget in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (3.2)\n",
      "Requirement already satisfied: torch in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from timm) (2.2.2+cu118)\n",
      "Requirement already satisfied: torchvision in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from timm) (0.17.2+cu118)\n",
      "Requirement already satisfied: pyyaml in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from timm) (0.27.0)\n",
      "Requirement already satisfied: safetensors in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from timm) (0.5.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from nnAudio) (1.13.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from nnAudio) (1.26.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (0.59.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (4.8.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from librosa) (1.0.8)\n",
      "Requirement already satisfied: packaging in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from lazy-loader>=0.1->librosa) (24.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (3.9.0)\n",
      "Requirement already satisfied: sympy in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torch->timm) (11.8.86)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: pycparser in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from jinja2->torch->timm) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ziga/miniconda3/envs/dl/lib/python3.12/site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install timm einops nnAudio librosa wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import zipfile\n",
    "import wget\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-04 14:36:50--  https://raw.githubusercontent.com/nttcslab/m2d/master/examples/portable_m2d.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15862 (15K) [text/plain]\n",
      "Saving to: ‘portable_m2d.py.1’\n",
      "\n",
      "portable_m2d.py.1   100%[===================>]  15.49K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2025-01-04 14:36:50 (8.04 MB/s) - ‘portable_m2d.py.1’ saved [15862/15862]\n",
      "\n",
      "--2025-01-04 14:36:51--  https://github.com/nttcslab/m2d/releases/download/v0.3.0/m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/589370928/0bdeb8a7-c3f3-44c5-afb9-9b9edaa3e861?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250104T133651Z&X-Amz-Expires=300&X-Amz-Signature=e453bd6577747f3acf210579c28aa287f18f6f1e62113516bb18cf28b23e8f71&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dm2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-01-04 14:36:51--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/589370928/0bdeb8a7-c3f3-44c5-afb9-9b9edaa3e861?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250104T133651Z&X-Amz-Expires=300&X-Amz-Signature=e453bd6577747f3acf210579c28aa287f18f6f1e62113516bb18cf28b23e8f71&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dm2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 319076219 (304M) [application/octet-stream]\n",
      "Saving to: ‘m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip’\n",
      "\n",
      "m2d_vit_base-80x100 100%[===================>] 304.29M  53.7MB/s    in 5.8s    \n",
      "\n",
      "2025-01-04 14:36:57 (52.8 MB/s) - ‘m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip’ saved [319076219/319076219]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download M2D model files\n",
    "!wget https://raw.githubusercontent.com/nttcslab/m2d/master/examples/portable_m2d.py\n",
    "!wget https://github.com/nttcslab/m2d/releases/download/v0.3.0/m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip\n",
    "\n",
    "# Extract the model weights\n",
    "with zipfile.ZipFile(\"../models/m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using 151 parameters, while dropped 9 out of 160 parameters from ../models/m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d/weights_ep69it3124-0.47929.pth\n",
      " (dropped: ['module.ar.runtime.to_spec.mel_basis', 'module.ar.runtime.to_spec.stft.wsin', 'module.ar.runtime.to_spec.stft.wcos', 'module.ar.runtime.to_spec.stft.window_mask', 'module.head.norm.running_mean'] ...)\n",
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PortableM2D(\n",
       "  (backbone): LocalViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (to_spec): MelSpectrogram(\n",
       "    Mel filter banks size = (80, 201), trainable_mel=False\n",
       "    (stft): STFT(n_fft=400, Fourier Kernel size=(201, 1, 400), iSTFT=False, trainable=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base M2D model\n",
    "from portable_m2d import PortableM2D\n",
    "\n",
    "# Initialize model without classification head (we'll add our own)\n",
    "model = PortableM2D(\n",
    "    weight_file='../models/m2d_vit_base-80x1001p16x16-221006-mr7_as_46ab246d/weights_ep69it3124-0.47929.pth',\n",
    "    num_classes=None  # Set to None to get embeddings instead of classification\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio preprocessing settings from the paper\n",
    "SAMPLE_RATE = model.cfg.sample_rate  # Use M2D's sample rate\n",
    "N_FFT = int(0.025 * SAMPLE_RATE)  # 25ms window\n",
    "HOP_LENGTH = int(0.010 * SAMPLE_RATE)  # 10ms hop\n",
    "N_MELS = 80\n",
    "F_MIN = 50\n",
    "F_MAX = 8000\n",
    "AUDIO_MEAN = -7.1\n",
    "AUDIO_STD = 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_file=None, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Initialize mel spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX\n",
    "        )\n",
    "        \n",
    "        # Load labels from CSV\n",
    "        self.labels_df = pd.read_csv('../data/genre_dataset.csv')\n",
    "        self.files = [self.data_dir / f for f in self.labels_df['path'].values]\n",
    "        self.labels = self.labels_df['genre'].values\n",
    "            \n",
    "        # Convert genre names to indices\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.labels = [self.label_to_idx[label] for label in self.labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        return waveform, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_classifier(model, train_loader, val_loader, num_classes, device='cuda', patience=10):\n",
    "    classifier = nn.Linear(3840, num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.1)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    best_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # For early stopping\n",
    "    best_val_acc = 0\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, leave=False)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embeddings = model(inputs)\n",
    "                embeddings = embeddings.mean(dim=1)\n",
    "            \n",
    "            outputs = classifier(embeddings)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                embeddings = model(inputs)\n",
    "                embeddings = embeddings.mean(dim=1)\n",
    "                outputs = classifier(embeddings)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.3f} | Train Acc: {train_acc:.3f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.3f} | Val Acc: {val_acc:.3f}%')\n",
    "        \n",
    "        # Save if it's the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            # Save both classifier and metadata\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': train_acc,\n",
    "            }, 'best_genre_classifier.pth')\n",
    "            print(f'New best model saved with validation accuracy: {val_acc:.3f}%')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch + 1} epochs without improvement')\n",
    "            break\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = AudioDataset('../data')\n",
    "\n",
    "# Split into train/val/test\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504142da5edd43d89c33adf4f3e9a0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be57e627a09043c5a86fe6ba64af62ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 3.922 | Train Acc: 32.184%\n",
      "Val Loss: 14.280 | Val Acc: 24.000%\n",
      "New best model saved with validation accuracy: 24.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb0c90ce0894e49a0449c3bd9c487c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 13.604 | Train Acc: 36.782%\n",
      "Val Loss: 13.202 | Val Acc: 56.000%\n",
      "New best model saved with validation accuracy: 56.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4a8fa6f00c419f9e8d3f03eceaed72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train Loss: 5.528 | Train Acc: 67.816%\n",
      "Val Loss: 10.130 | Val Acc: 40.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15a47c6ca1e4ed19ee1a481f0394271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train Loss: 4.241 | Train Acc: 67.816%\n",
      "Val Loss: 1.186 | Val Acc: 76.000%\n",
      "New best model saved with validation accuracy: 76.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dc23e3fc4e401181ca51885540eff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train Loss: 0.284 | Train Acc: 93.103%\n",
      "Val Loss: 1.118 | Val Acc: 68.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5f72cddb3d42a8a0274a1f466a9ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train Loss: 0.328 | Train Acc: 94.253%\n",
      "Val Loss: 0.801 | Val Acc: 88.000%\n",
      "New best model saved with validation accuracy: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d585ad60d30943878caf44bfb0d269a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train Loss: 0.039 | Train Acc: 98.851%\n",
      "Val Loss: 0.997 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ec70e4afed41d9a957f340c78fddfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train Loss: 0.022 | Train Acc: 98.851%\n",
      "Val Loss: 0.764 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adab7876f4614805aec6f0a916c62fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train Loss: 0.041 | Train Acc: 97.701%\n",
      "Val Loss: 0.802 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4f210c58354a67ab96e87329faf28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train Loss: 0.007 | Train Acc: 100.000%\n",
      "Val Loss: 0.938 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b974162e5d49e283396f93ee6aa0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train Loss: 0.003 | Train Acc: 100.000%\n",
      "Val Loss: 0.891 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91d00a5b095442380c28a37f38d5d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.910 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f279579134024e948b01edfa291050a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.915 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f89f79e6194a569efd3a6ccceb2b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.928 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa93879fcff4459f89faf635b84d1659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.923 | Val Acc: 88.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a40e61f33e499f9b8c1583a4f24084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.920 | Val Acc: 88.000%\n",
      "Early stopping after 16 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "num_classes = len(dataset.label_to_idx)\n",
    "classifier = train_linear_classifier(model, train_loader, val_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.923%\n"
     ]
    }
   ],
   "source": [
    "def test_classifier(model, classifier, test_loader, device='cuda'):\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            embeddings = model(inputs)\n",
    "            embeddings = embeddings.mean(dim=1)\n",
    "            outputs = classifier(embeddings)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_acc = 100. * correct / total\n",
    "    print(f'Test Accuracy: {test_acc:.3f}%')\n",
    "    return test_acc\n",
    "\n",
    "# Test the classifier\n",
    "test_accuracy = test_classifier(model, classifier, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets exported to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Export train, val, and test datasets to CSV\n",
    "train_df = dataset.labels_df.iloc[train_dataset.indices]\n",
    "val_df = dataset.labels_df.iloc[val_dataset.indices]\n",
    "test_df = dataset.labels_df.iloc[test_dataset.indices]\n",
    "\n",
    "train_df.to_csv('../data/train_dataset.csv', index=False)\n",
    "val_df.to_csv('../data/val_dataset.csv', index=False)\n",
    "test_df.to_csv('../data/test_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets exported to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GenreCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_file=None, transform=None, for_cnn=False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.for_cnn = for_cnn\n",
    "        \n",
    "        # Initialize mel spectrogram transform\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_fft=N_FFT,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX\n",
    "        )\n",
    "        \n",
    "        # Normalization values\n",
    "        self.spec_mean = AUDIO_MEAN\n",
    "        self.spec_std = AUDIO_STD\n",
    "        \n",
    "        # Load labels from CSV\n",
    "        self.labels_df = pd.read_csv('../data/genre_dataset.csv')\n",
    "        self.files = [self.data_dir / f for f in self.labels_df['path'].values]\n",
    "        self.labels = self.labels_df['genre'].values\n",
    "            \n",
    "        # Convert genre names to indices\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.labels = [self.label_to_idx[label] for label in self.labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio file\n",
    "        waveform, sr = torchaudio.load(self.files[idx])\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if sr != SAMPLE_RATE:\n",
    "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Convert to mono if stereo\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "        if self.for_cnn:\n",
    "            # Take a fixed-length segment (6 seconds like M2D)\n",
    "            segment_length = 6 * SAMPLE_RATE\n",
    "            if waveform.shape[1] > segment_length:\n",
    "                start = torch.randint(0, waveform.shape[1] - segment_length, (1,))\n",
    "                waveform = waveform[:, start:start + segment_length]\n",
    "            else:\n",
    "                # Pad if too short\n",
    "                padding = segment_length - waveform.shape[1]\n",
    "                waveform = F.pad(waveform, (0, padding))\n",
    "            \n",
    "            # Get mel spectrogram\n",
    "            spec = self.mel_spec(waveform)\n",
    "            # Convert to dB scale\n",
    "            spec = torchaudio.transforms.AmplitudeToDB()(spec)\n",
    "            # Normalize\n",
    "            spec = (spec - self.spec_mean) / self.spec_std\n",
    "            return spec, self.labels[idx]\n",
    "        \n",
    "        return waveform, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, val_loader, device='cuda', patience=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    best_val_acc = 0\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc='Training'):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, leave=False)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.3f} | Train Acc: {train_acc:.3f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.3f} | Val Acc: {val_acc:.3f}%')\n",
    "        \n",
    "        # Save if it's the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'train_acc': train_acc,\n",
    "            }, 'best_cnn_classifier.pth')\n",
    "            print(f'New best model saved with validation accuracy: {val_acc:.3f}%')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        # Early stopping check\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping after {epoch + 1} epochs without improvement')\n",
    "            break\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn(model, test_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_acc = 100. * correct / total\n",
    "    print(f'Test Accuracy: {test_acc:.3f}%')\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spectrogram shape: torch.Size([32, 1, 80, 601])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696a897882fa4967b7090e39155cf8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688e813bb57544d1a1c68c5a0dd1caf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train Loss: 1.569 | Train Acc: 32.184%\n",
      "Val Loss: 1.564 | Val Acc: 16.000%\n",
      "New best model saved with validation accuracy: 16.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462e5fc389ff49f0a37a85a53390b271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Loss: 1.413 | Train Acc: 40.230%\n",
      "Val Loss: 1.477 | Val Acc: 40.000%\n",
      "New best model saved with validation accuracy: 40.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad234400b6104cf9b66fef31e993f121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Train Loss: 1.283 | Train Acc: 49.425%\n",
      "Val Loss: 1.347 | Val Acc: 48.000%\n",
      "New best model saved with validation accuracy: 48.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7624d9bb9e4d30be4d57e70fb39ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Train Loss: 1.199 | Train Acc: 55.172%\n",
      "Val Loss: 1.235 | Val Acc: 52.000%\n",
      "New best model saved with validation accuracy: 52.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19261837a6141938465b7394615fe27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Train Loss: 1.115 | Train Acc: 57.471%\n",
      "Val Loss: 1.181 | Val Acc: 52.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d07dd6007a44599001fb20fc214a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Train Loss: 1.090 | Train Acc: 59.770%\n",
      "Val Loss: 1.628 | Val Acc: 32.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5c00542fd4411ea01bb9d1942bf808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Train Loss: 1.116 | Train Acc: 57.471%\n",
      "Val Loss: 2.082 | Val Acc: 32.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1946a87188e4de090f171c65d7b5ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Train Loss: 1.086 | Train Acc: 56.322%\n",
      "Val Loss: 1.466 | Val Acc: 44.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3af2df0a5f4810b9748aee3a394caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Train Loss: 0.979 | Train Acc: 62.069%\n",
      "Val Loss: 1.364 | Val Acc: 52.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027e563baad54fbea751b13cdbd6e18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Train Loss: 0.969 | Train Acc: 58.621%\n",
      "Val Loss: 1.187 | Val Acc: 60.000%\n",
      "New best model saved with validation accuracy: 60.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8685dd3f3a474288dc396d39dcedd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Train Loss: 0.956 | Train Acc: 62.069%\n",
      "Val Loss: 1.102 | Val Acc: 60.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8947332ba9d244bda38b12b3874a5a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Train Loss: 0.889 | Train Acc: 64.368%\n",
      "Val Loss: 1.214 | Val Acc: 56.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0834775e5e64409f8109a4baf7f00f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Train Loss: 0.916 | Train Acc: 60.920%\n",
      "Val Loss: 1.383 | Val Acc: 44.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4343baa533f4609bf0602fc5ae6d79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Train Loss: 0.913 | Train Acc: 60.920%\n",
      "Val Loss: 1.179 | Val Acc: 56.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab10ffb5bfa24638bdfba16ef962ddbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Train Loss: 0.898 | Train Acc: 63.218%\n",
      "Val Loss: 1.340 | Val Acc: 48.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a8d0e5c4944af1be705219a4783a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Train Loss: 0.919 | Train Acc: 66.667%\n",
      "Val Loss: 1.425 | Val Acc: 36.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae21e3d2c5e54f8f9d5d06e63ee92d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "Train Loss: 0.891 | Train Acc: 65.517%\n",
      "Val Loss: 1.259 | Val Acc: 52.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920198705a2430abc46180fa96f9c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "Train Loss: 0.853 | Train Acc: 68.966%\n",
      "Val Loss: 1.286 | Val Acc: 48.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5279522428804e009645fb7c645f4711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n",
      "Train Loss: 0.844 | Train Acc: 71.264%\n",
      "Val Loss: 1.408 | Val Acc: 48.000%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afa31a05ab64f4aade61768d153159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "Train Loss: 0.876 | Train Acc: 66.667%\n",
      "Val Loss: 1.240 | Val Acc: 60.000%\n",
      "Early stopping after 20 epochs without improvement\n",
      "Test Accuracy: 61.538%\n"
     ]
    }
   ],
   "source": [
    "# Create datasets for CNN\n",
    "cnn_dataset = AudioDataset('../data', for_cnn=True)\n",
    "train_dataset_cnn, val_dataset_cnn, test_dataset_cnn = torch.utils.data.random_split(\n",
    "    cnn_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Check input shape\n",
    "batch = next(iter(train_loader_cnn))\n",
    "inputs, _ = batch\n",
    "print(\"Input spectrogram shape:\", inputs.shape)\n",
    "\n",
    "# Initialize and train CNN\n",
    "num_classes = len(dataset.label_to_idx)\n",
    "cnn_model = GenreCNN(num_classes).to(device)\n",
    "cnn_model = train_cnn(cnn_model, train_loader_cnn, val_loader_cnn)\n",
    "\n",
    "# Test the CNN classifier\n",
    "test_accuracy_cnn = test_cnn(cnn_model, test_loader_cnn, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
